{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b231bca",
   "metadata": {},
   "source": [
    "# 02_preprocessing.ipynb – Datenbereinigung und Preprocessing\n",
    "\n",
    "**Ziel:** Die Amazon Reviews Daten werden bereinigt, normalisiert und als preprocessed Datensätze gespeichert.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e728257e",
   "metadata": {},
   "source": [
    "## 1. Setup und Laden der Rohdaten\n",
    "Wir laden die Rohdaten aus dem Ordner `data/raw`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea11e8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automotive: 1000000 Einträge geladen.\n",
      "Video_Games: 1000000 Einträge geladen.\n",
      "Books: 1000000 Einträge geladen.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "RAW_DIR = Path('../data/raw')\n",
    "PRE_DIR = Path('../data/preprocessed')\n",
    "PRE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CATEGORIES = ['Automotive', 'Video_Games', 'Books']\n",
    "dfs = {}\n",
    "N = 1_000_000            # Zeilenlimit pro Kategorie\n",
    "\n",
    "for cat in CATEGORIES:\n",
    "    data = []\n",
    "    with open(RAW_DIR / f'{cat}.jsonl', 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= N:          # nach N Zeilen abbrechen\n",
    "                break\n",
    "            data.append(json.loads(line))\n",
    "    df = pd.DataFrame(data)\n",
    "    dfs[cat] = df\n",
    "    print(f'{cat}: {df.shape[0]} Einträge geladen.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ba9f87",
   "metadata": {},
   "source": [
    "## 2. Bereinigung: Leere Texte, ungültige Ratings, Duplikate entfernen\n",
    "Wir filtern alle Rezensionen mit leerem Text, Rating außerhalb 1–5 und entfernen Duplikate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1e7000f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automotive: 1000000 → 996426 nach Bereinigung.\n",
      "Video_Games: 1000000 → 995507 nach Bereinigung.\n",
      "Books: 1000000 → 997498 nach Bereinigung.\n"
     ]
    }
   ],
   "source": [
    "def clean_df(df):\n",
    "    # Entferne Ratings außerhalb 1–5\n",
    "    df = df[df['rating'].isin([1, 2, 3, 4, 5])]\n",
    "    # Entferne leere Texte\n",
    "    df = df[df['text'].astype(str).str.strip() != '']\n",
    "    # Entferne Duplikate\n",
    "    if set(['user_id', 'asin', 'text']).issubset(df.columns):\n",
    "        df = df.drop_duplicates(subset=['user_id', 'asin', 'text'])\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "for cat in CATEGORIES:\n",
    "    before = dfs[cat].shape[0]\n",
    "    dfs[cat] = clean_df(dfs[cat])\n",
    "    after = dfs[cat].shape[0]\n",
    "    print(f'{cat}: {before} → {after} nach Bereinigung.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfadc01",
   "metadata": {},
   "source": [
    "## 3. Textnormalisierung: Kleinbuchstaben, Sonderzeichen entfernen\n",
    "Wir wandeln alle Texte in Kleinbuchstaben um und entfernen Sonderzeichen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c0cf06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automotive: Texte normalisiert.\n",
      "Video_Games: Texte normalisiert.\n",
      "Books: Texte normalisiert.\n"
     ]
    }
   ],
   "source": [
    "def normalize_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'<.*?>', ' ', text)      # HTML-Tags\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)    # Sonderzeichen\n",
    "    text = re.sub(r'\\d+', ' ', text)        # Zahlen\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "for cat in CATEGORIES:\n",
    "    dfs[cat]['text'] = dfs[cat]['text'].apply(normalize_text)\n",
    "    dfs[cat]['title'] = dfs[cat]['title'].apply(normalize_text)\n",
    "    print(f'{cat}: Texte normalisiert.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1260114",
   "metadata": {},
   "source": [
    "## 4. Speichern der bereinigten Datensätze\n",
    "Wir speichern die bereinigten Daten als .jsonl-Dateien im Ordner `data/preprocessed`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0df6c361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automotive: gespeichert unter ..\\data\\preprocessed\\Automotive_preprocessed.jsonl\n",
      "Video_Games: gespeichert unter ..\\data\\preprocessed\\Video_Games_preprocessed.jsonl\n",
      "Books: gespeichert unter ..\\data\\preprocessed\\Books_preprocessed.jsonl\n"
     ]
    }
   ],
   "source": [
    "for cat, df in dfs.items():\n",
    "    out_path = PRE_DIR / f'{cat}_preprocessed.jsonl'\n",
    "    df.to_json(out_path, orient='records', lines=True, force_ascii=False)\n",
    "    print(f'{cat}: gespeichert unter {out_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f38c789",
   "metadata": {},
   "source": [
    "---\n",
    "**Hinweis:** Weitere Schritte wie Lemmatisierung, Stopword-Entfernung oder Tokenisierung können später ergänzt werden, z. B. mit spaCy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
