# Modellwahl-Analyse: BERT vs. SVM

**Datum:** 30. Juli 2025  
**Entscheider:** Jan Serebranski  
**Baseline-Ergebnisse:** 71.5% Accuracy, Klassenungleichgewicht identifiziert

---

## ğŸ¯ **1. Entscheidungskriterien**

### **Projektanforderungen:**
- **Accuracy â‰¥ 70%** âœ… (Baseline bereits erfÃ¼llt)
- **â‰¥20% Verbesserung Ã¼ber Mehrheitsklasse** âŒ (Nur 2.4-5.8% erreicht)
- **Rechenzeit â‰¤ 4h** auf Colab GPU
- **Reproduzierbarkeit** und **Dokumentation**

### **Identifizierte Probleme:**
- **Klassenungleichgewicht:** Rating 5 dominiert (60-70%)
- **Schwache Performance bei Rating 2-4:** Recall < 20%
- **Einfache Baseline:** Naive Bayes kann komplexe Muster nicht erfassen

---

## ğŸ¤– **2. Modell-Optionen**

### **Option A: BERT/DistilBERT Fine-Tuning**
**Vorteile:**
- âœ… **Beste TextverstÃ¤ndnis:** Kann Kontext und Nuancen erfassen
- âœ… **State-of-the-art** fÃ¼r Sentimentanalyse
- âœ… **Class Weights** einfach implementierbar
- âœ… **Transfer Learning:** Vorgelerntes Wissen nutzen
- âœ… **Gute Performance** bei unausgeglichenen Klassen

**Nachteile:**
- âŒ **Hohe Rechenzeit:** 2-4h Training
- âŒ **GPU erforderlich:** Colab notwendig
- âŒ **KomplexitÃ¤t:** Mehr Hyperparameter
- âŒ **Overfitting-Risiko** bei kleinen Klassen

### **Option B: SVM mit RBF-Kernel**
**Vorteile:**
- âœ… **Schnell:** 30-60min Training
- âœ… **Einfach:** Weniger Hyperparameter
- âœ… **Class Weights** verfÃ¼gbar
- âœ… **Robust:** Weniger Overfitting-Risiko
- âœ… **CPU-basiert:** Keine GPU nÃ¶tig

**Nachteile:**
- âŒ **EingeschrÃ¤nktes TextverstÃ¤ndnis:** TF-IDF Features
- âŒ **Feature Engineering** kritisch
- âŒ **MÃ¶glicherweise schlechtere Performance** als BERT

---

## ğŸ“Š **3. Vergleichsmatrix**

| Kriterium | Gewichtung | BERT | SVM | Gewinner |
|-----------|------------|------|-----|----------|
| **Accuracy-Verbesserung** | 40% | 9/10 | 6/10 | **BERT** |
| **Rechenzeit** | 20% | 4/10 | 9/10 | **SVM** |
| **Klassenungleichgewicht** | 25% | 9/10 | 7/10 | **BERT** |
| **ImplementierungskomplexitÃ¤t** | 15% | 5/10 | 8/10 | **SVM** |

**Gesamtscore:** BERT = 7.4/10, SVM = 7.2/10

---

## ğŸ” **4. Literatur-Review**

### **BERT fÃ¼r Sentimentanalyse:**
- **Devlin et al. (2019):** BERT erreicht SOTA bei Sentiment Tasks
- **Sun et al. (2019):** DistilBERT 97% der BERT-Performance bei 60% der GrÃ¶ÃŸe
- **Class Imbalance:** BERT mit Class Weights zeigt 15-25% Verbesserung

### **SVM fÃ¼r Sentimentanalyse:**
- **Joachims (1998):** SVM effektiv fÃ¼r Textklassifikation
- **Class Imbalance:** SMOTE + SVM zeigt 10-15% Verbesserung
- **Feature Engineering:** N-Grams + Sentiment Lexicons wichtig

---

## ğŸš€ **5. Empfehlung**

### **âœ… Empfohlen: BERT/DistilBERT Fine-Tuning**

**BegrÃ¼ndung:**
1. **HÃ¶chstes Verbesserungspotential** fÃ¼r Klassenungleichgewicht
2. **State-of-the-art Performance** fÃ¼r Sentimentanalyse
3. **Class Weights** einfach implementierbar
4. **Rechenzeit akzeptabel** (â‰¤4h auf Colab)
5. **Projektziele** erfordern deutliche Verbesserung

### **Implementierungsplan:**
1. **DistilBERT** (kleiner, schneller als BERT)
2. **Class Weights** fÃ¼r ausgewogene Gewichtung
3. **Learning Rate:** 2e-5 bis 5e-5
4. **Batch Size:** 16-32 (GPU-Memory)
5. **Epochs:** 3-5 (Early Stopping)

---

## ğŸ“‹ **6. Fallback-Plan**

### **Wenn BERT zu langsam/komplex:**
1. **SVM mit RBF-Kernel** als Alternative
2. **Feature Engineering:** N-Grams, Sentiment Lexicons
3. **SMOTE** fÃ¼r Klassenausgleich
4. **Grid Search** fÃ¼r Hyperparameter

### **Wenn BERT nicht konvergiert:**
1. **Kleinere DatensÃ¤tze** testen
2. **Learning Rate** reduzieren
3. **Batch Size** anpassen
4. **Fallback auf SVM**

---

## âœ… **7. Beschluss**

**Entscheidung:** **BERT/DistilBERT Fine-Tuning**

**NÃ¤chste Schritte:**
1. **Aufgabe 4.2:** Fine-Tuning Setup (`train_hf.py`)
2. **Class Weights** implementieren
3. **Hyperparameter-Tuning** mit Optuna
4. **FrÃ¼he Evaluation** nach 1-2 Epochs

**Zeitrahmen:** 1-2 Wochen fÃ¼r Training und Evaluation

---

**Datum:** 30. Juli 2025 